
--------------PRE-, TRAINING, TESTING--------------------

import numpy as np
import matplotlib.pyplot as plt
import scipy.sparse as sparse
from scipy import stats
from functions import triangle, sinusoid


linewidth = 3;
fontsize = 14;
fontweight = 'bold';


N = 1000;
p = 0.1;
g = 1.5;				# g greater than 1 leads to chaotic networks.
alpha = 1.0;
nsecs = 1440;
dt = 0.1;
learn_every = 2;

scale = 1.0/np.sqrt(p*N);
rvs = stats.norm().rvs
M = sparse.random(N,N,p, data_rvs = rvs).todense()*g*scale

nRec2Out = int(N)
#nRec2Control = int(N/2)
neurons_recorded = 10

"""
% Allow output and control units to start with different ICs.  If you set beta greater than zero, then y will look
% different than z but still drive the network with the appropriate frequency content (because it will be driven with
% z).  A value of beta = 0 shows that the learning rules produce extremely similar signals for both z(t) and y(t),
% despite having no common pre-synaptic inputs.  Keep in mind that the vector norm of the output weights is 0.1-0.2 when
% finished, so if you make beta too big, things will eventually go crazy and learning won't converge.
%beta = 0.1;	
"""

beta = 1.0
wo = beta*np.random.randn(nRec2Out,1)/np.sqrt(N)		# synaptic strengths from internal pool to output unit
dwo = np.zeros((nRec2Out,1))
#wc = beta*np.random.randn(nRec2Control, 1)/np.sqrt(N/2) 	# synaptic strengths from internal pool to control unit
#dwc = np.zeros((nRec2Control, 1))

wf = 2.0*(np.random.rand(N,1)-0.5) 	# the feedback now comes from the control unit as opposed to the output

# Deliberatley set the pre-synaptic neurons to nonoverlapping between the output and control units.
zidxs = np.arange(round(N))
#yidxs = np.arange(round(N/2), N)	 

print('   N: ' + str(N))
print('   g: '+str(g))
print('   p: '+ str(p))
print('   nRec2Out: '+ str(nRec2Out))
#print('   nRec2Control: '+ str(nRec2Control))
print('   alpha: '+ str(alpha))
print('   nsecs: '+ str(nsecs))
print('   learn_every: '+ str(learn_every))

pre_training=np.arange(0, nsecs/2, dt);

simtime = np.arange(0, nsecs, dt)
simtime_len = len(simtime)
simtime2 = np.arange(1*nsecs, 2*nsecs, dt)

amp = 1.3;
freq = 1/60;
ft = (amp/1.0)*np.sin(1.0*np.pi*freq*simtime) + (amp/2.0)*np.sin(2.0*np.pi*freq*simtime) + (amp/6.0)*np.sin(3.0*np.pi*freq*simtime) + (amp/3.0)*np.sin(4.0*np.pi*freq*simtime);
ft = ft/1.5;

ft2 = (amp/1.0)*np.sin(1.0*np.pi*freq*simtime2) + (amp/2.0)*np.sin(2.0*np.pi*freq*simtime2) + (amp/6.0)*np.sin(3.0*np.pi*freq*simtime2) + (amp/3.0)*np.sin(4.0*np.pi*freq*simtime2);
ft2 = ft2/1.5;

ft = triangle(simtime, amp, freq)

wo_len = np.zeros((1,simtime_len))  
wc_len = np.zeros((1,simtime_len))
zt = np.zeros((1,simtime_len))
zpre=np.zeros((1,simtime_len));
#yt = np.zeros((1,simtime_len))
zpt = np.zeros((1,simtime_len))
#ypt = np.zeros((1,simtime_len))

J_GG=np.random.normal(0,1/(p*N),(N,N));
J_Gz=np.random.uniform(-1,1,(N,1));

x0 = 0.5*np.random.randn(N,1);
z0 = 0.5*np.random.randn(1,1);
y0 = 0.5*np.random.randn(1,1);

x = x0; 
r = np.tanh(x);
z = z0; 
y = y0;

plt.figure()
ti = 0;
Pz = (1.0/alpha)*np.identity(nRec2Out);
#Py = (1.0/alpha)*np.identity(nRec2Control);



neurons = np.zeros((neurons_recorded,1))
pick_random_neurons = np.random.randint(0, N/2, size = neurons_recorded)


#pre-training

n1_pre = np.array([])
n10_pre = np.array([])
n50_pre = np.array([])
n100_pre = np.array([])
n200_pre = np.array([])
n500_pre = np.array([])
n600_pre = np.array([])
n700_pre = np.array([])
n800_pre = np.array([])
n900_pre = np.array([])

for t in simtime:				# don't want to subtract time in indices
     
    if ti%(nsecs/2) == 0:
    	print('time: ' + str(t) +'.');
      #  plt.figure('fig.training');
    	plt.subplot(211);
    	plt.plot(simtime, ft.T, linewidth = linewidth, color = 'green');
    	plt.plot(simtime, zpre.T, linewidth = linewidth, color = 'red');
    	#plt.plot(simtime, ypre.T, linewidth = linewidth, color = 'magenta'); 
    	plt.title('pre-train', fontsize = fontsize, fontweight = fontweight);
    	plt.xlabel('time (ms)', fontsize = fontsize, fontweight = fontweight);
    	plt.ylabel('f, z', fontsize = fontsize, fontweight = fontweight);
    	plt.legend(['f', 'z']);
    	
    	plt.subplot(212);
    	plt.plot(simtime, wo_len.T, linewidth = linewidth); 
    	#plt.plot(simtime, wc_len.T, linewidth = linewidth, color = 'green'); 
    	plt.xlabel('time (ms)', fontsize = fontsize, fontweight = fontweight);
    	plt.ylabel('|w_o|', fontsize = fontsize, fontweight = fontweight);
    	plt.legend(['|w_o|']);	
    
    	plt.pause(0.5);	
    
    # sim, so x(t) and r(t) are created.
    x = np.add(np.add(np.dot((1.0-dt), x), np.dot(M, np.dot(r,dt))), np.dot(wf,np.dot(z,dt))); # note the y here.
    #x=(1.0-dt)*x + J_GG*(r*dt) + J_Gz*(y*dt)
    n1_pre = np.append(n1_pre, x[1])
    n10_pre = np.append(n10_pre, x[50])
    n50_pre = np.append(n50_pre, x[1])
    n100_pre = np.append(n100_pre, x[100])
    n200_pre = np.append(n200_pre, x[20])
    n500_pre = np.append(n500_pre, x[500])
    n600_pre = np.append(n600_pre, x[600])
    n700_pre = np.append(n700_pre, x[700])
    n800_pre = np.append(n800_pre, x[800])
    n900_pre = np.append(n900_pre, x[900])

    r = np.tanh(x);
    rz = r[zidxs];			#the neurons that project to the output
    #ry = r[yidxs];			#the neurons that project to the control unit
    z = np.dot(wo.T,rz);
    #y = wc.T*ry;
    
    zpre.T[ti] = z;
   # ypre.T[ti] = y;
    wo_len[0,ti] = np.sqrt(np.dot(wo.T, wo));	
    #wc_len[0,ti] = np.sqrt(np.dot(wc.T, wc));
    ti += 1;    

error_avg_pre = np.sum(np.abs(zpre-ft2))/simtime_len
print('Pre-training MAE: ' + str(error_avg_pre))



#FORCE learning
ti=0;
for t in simtime:
    
    if ti%(nsecs/2) == 0:
    	print('time: ' + str(t) +'.');
    	plt.subplot(211);
    	plt.plot(simtime, ft.T, linewidth = linewidth, color = 'green');
    	plt.plot(simtime, zt.T, linewidth = linewidth, color = 'red');
    	#plt.plot(simtime, yt.T, linewidth = linewidth, color = 'magenta'); 
    	plt.title('training', fontsize = fontsize, fontweight = fontweight);
    	plt.xlabel('time (ms)', fontsize = fontsize, fontweight = fontweight);
    	plt.ylabel('f, z', fontsize = fontsize, fontweight = fontweight);
    	plt.legend(['f', 'z']);
    	
    	plt.subplot(212);
    	plt.plot(simtime, wo_len.T, linewidth = linewidth); 
    	#plt.plot(simtime, wc_len.T, linewidth = linewidth, color = 'green'); 
    	plt.xlabel('time (ms)', fontsize = fontsize, fontweight = fontweight);
    	plt.ylabel('|w_o|', fontsize = fontsize, fontweight = fontweight);
    	plt.legend(['|w_o|'])
    	plt.pause(0.5)
        
    	if ti + nsecs/2 < len(simtime):
            plt.clf()
    
    # sim, so x(t) and r(t) are created.
    x = np.add(np.add(np.dot((1.0-dt), x), np.dot(M, np.dot(r,dt))), np.dot(wf,np.dot(z,dt))); # note the y here.
    r = np.tanh(x);
    rz = r[zidxs]   		# the neurons that project to the output
   # ry = r[yidxs]			# the neurons that project to the control unit
    z = np.dot(wo.T,rz);
    #y = wc.T*ry;
    
    if ti % learn_every == 0:
    	# update inverse correlation matrix for the output unit
    	kz = np.dot(Pz,rz);
    	rPrz = np.dot(rz.T,kz);
    	cz = np.divide(1.0,np.add(1.0, rPrz));
    	Pz = Pz - np.dot(kz,np.dot(kz,cz).T);    
    	# update the error for the linear readout
    	e = np.subtract(z,ft[ti]);
    	# update the output weights
    	dwo = np.dot(np.dot(-float(e),kz),cz);
    	wo += dwo;
    
    	# update inverse correlation matrix for the control unit
    	#ky = Py*ry;
    	#rPry = ry.T*ky;
    	#cy = 1.0/(1.0 + rPry);
    	#Py -= ky*(ky*cy).T;    
    
    	### NOTE WE USE THE OUTPUT'S ERROR ###
    	# update the output weights
    	#dwc = -float(e)*ky*cy;
    	#wc += dwc;
        
    
    # Store the output of the system.
    zt[0,ti] = z;
    #yt[0,ti] = y;
    wo_len[0,ti] = np.sqrt(np.dot(wo.T, wo));	
    #wc_len[0,ti] = np.sqrt(np.dot(wc.T, wc));
    
    neurons = np.append(neurons, np.array(rz[pick_random_neurons]), axis = 1)
    
    ti += 1;	
        

error_avg = np.sum(np.abs(zt-ft))/simtime_len;
print('Training MAE: ' + str(error_avg));    
print('Now testing... please wait.');    

# Now test. 
ti = 0;

neurons_test = np.zeros((neurons_recorded,1))

for t in simtime:				# don't want to subtract time in indices
    if ti%(nsecs/2) == 0:
    	 print('time: ' + str(t) +'.');
       #  plt.figure('fig.training');
    	 plt.subplot(211);
    	 plt.plot(simtime, ft.T, linewidth = linewidth, color = 'green');
    	 plt.plot(simtime, zpt.T, linewidth = linewidth, color = 'red');
    	 #plt.plot(simtime, ypt.T, linewidth = linewidth, color = 'magenta'); 
    	 plt.title('Test', fontsize = fontsize, fontweight = fontweight);
    	 plt.xlabel('time (ms)', fontsize = fontsize, fontweight = fontweight);
    	 plt.ylabel('f, z', fontsize = fontsize, fontweight = fontweight);
    	 plt.legend(['f', 'z']);
    	
    	 plt.subplot(212);
    	 plt.plot(simtime, wo_len.T, linewidth = linewidth); 
    	 #plt.plot(simtime, wc_len.T, linewidth = linewidth, color = 'green'); 
    	 plt.xlabel('time(ms)', fontsize = fontsize, fontweight = fontweight);
    	 plt.ylabel('|w_o|', fontsize = fontsize, fontweight = fontweight);
    	 plt.legend(['|w_o|']);	
    
       
    	 plt.pause(0.5);	# sim, so x(t) and r(t) are created.       
    # sim, so x(t) and r(t) are created.
    x = np.add(np.add(np.dot((1.0-dt), x), np.dot(M, np.dot(r,dt))), np.dot(wf,np.dot(z,dt))); # note the y here.
    

    r = np.tanh(x);
    rz = r[zidxs];			#the neurons that project to the output
    #ry = r[yidxs];			#the neurons that project to the control unit
    z = wo.T*rz;
    #y = wc.T*ry;
    
    zpt.T[ti] = z;
    #ypt.T[ti] = y;
    
    neurons_test = np.append(neurons_test, np.array(rz[pick_random_neurons]), axis = 1)
    
    ti += 1;    

error_avg = np.sum(np.abs(zpt-ft2))/simtime_len
print('Testing MAE: ' + str(error_avg))



------------------END-----------------------








--------------------OLD CODE BELOW NOT WORKING----------------------------
---------------------------------------------------------------------
#unfinished: not sure if there is learning: e_min is not decreasing
import numpy as np;
import matplotlib.pyplot as plt;

g_GG=1.5;
g_Gz=1;
g_GF=0;
g_FF=1.2;
g_FG=1;
alpha=1;
N_G=10;
N_F=2;
p_GG=0.1;
p_Z=1;
p_FG=0.025;
p_FF=0.25;
p_GF=0.25;
T=1500;
np.random.seed(1);

P=np.zeros((T,N_G,N_G));
P[0]=np.divide(np.identity(N_G),alpha);
w=np.zeros((T,N_G));
w_nonzero=np.random.normal(0,1/(p_Z*N_G),(N_G));
for i in range(N_G):
    w[0][i]=np.random.choice([0,w_nonzero[i]]);


J_GG=np.random.normal(0,1/(p_Z*N_G),(N_G,N_G));
J_Gz=np.random.uniform(-1,1,(N_G));
J_FG=np.random.normal(0,1/(p_FG*N_G),(N_F,N_G));
J_FF=np.random.normal(0,1/(p_FF*N_F),(N_F,N_F));
J_GF=np.random.normal(0,1/(p_GF*N_F),(N_G,N_F));
tau=1;

#x0=0.1

#periodic function D
nsecs=T

simtime = np.arange(0, nsecs, tau)
simtime_len = len(simtime)
simtime2 = np.arange(1*nsecs, 2*nsecs, tau)

amp = 1.3;
freq = 1/60;
ft = (amp/1.0)*np.sin(1.0*np.pi*freq*simtime) + (amp/2.0)*np.sin(2.0*np.pi*freq*simtime) + (amp/6.0)*np.sin(3.0*np.pi*freq*simtime) + (amp/3.0)*np.sin(4.0*np.pi*freq*simtime);
ft = ft/1.5;




def first_sigma_x(d1=0):
    first_sigma_x=1;
    xt=np.ones(N_G);


    for i in range(N_G):

            first_sigma_x=first_sigma_x+g_GG*J_GG[d1-1][i]*np.tanh(xt[i]);
    return first_sigma_x;

def first_sigma_y(d1=0):
    first_sigma_y=1;
    yt=np.ones(N_F);
    for  i in range(1,N_F):
            first_sigma_y=first_sigma_y+g_FF*J_FF[d1-1][i]*np.tanh(yt[i-1]);
    return first_sigma_y;


def second_sigma_y(d1=0):
    second_sigma_y=1;
    xt=np.ones(N_G);
    for i in range(1,N_G):
        second_sigma_y=second_sigma_y+g_FG*J_FG[d1-1][i]*np.tan(xt[i]);
    return second_sigma_y ;

def ya(tau=tau):
    yt=np.ones(N_F);
    for i in range(1,N_F):
        yt[i]=yt[i-1]+(-yt[i-1]+first_sigma_y(i-1)+second_sigma_y(i-1))/tau;
    return yt;

def second_sigma_x(d1=0):
    yt=ya();
    second_sigma_x=1;
    for i in range(1,N_F):

        second_sigma_x=second_sigma_x+np.dot(np.dot(g_GF,J_GF[d1-1][i]),np.tanh(yt[i-1]));
        
    return second_sigma_x;

def xi(t=0,tau=tau):
    xt=np.ones(N_G);
    r=np.ones(N_G);
    z=np.zeros(T);

    for i in range(1,N_G):
        xt[i]=xt[i-1]+(-xt[i-1]+first_sigma_x(i)+g_Gz*J_Gz[i-1]*z[t-1]+second_sigma_x(i))/tau;
        r[i]=(np.tanh(xt[i]));
    return r;   

#N_I=0, so the last term is omitted
def g_GG_simul(N_G=N_G,N_F=N_F,tau=tau,T=T,ft=ft,P=P,w=w):
    # yt=np.ones(N_F);

    # xt=np.ones(N_G);
    # r=np.ones(N_G);
    z=np.zeros(T);
    #P=np.ones(T);
    e_min=np.zeros(T);
  #  e_min[0]=alpha*(f[1]-f[0])
    #loop for z
    for t in range(1,T):        

        
    #same thing as those fxns for sigma, y, and x
    #initialize sigmas in the two formula
        # first_sigma_x=1;
        # first_sigma_y=1;
        # second_sigma_y=1;
        # second_sigma_x=1;
        
        # if t>0:
           # #loop for xi and ri
           # for i in range(1,N_G):   
           #     #loop for 1st sigma x
           #     for j in range(N_G):
           #         first_sigma_x=first_sigma_x+g_GG*J_GG[i-1][j]*np.tanh(xt[j]);
           #         #loop for ya and 2nd sigma x
           #         for n in range(N_F):
           #             #loop for 2nd sigma y
           #             for m in range(N_G):
           #                 second_sigma_y=second_sigma_y+g_FG*J_FG[n][m]*np.tan(xt[m]);
           #             #loop for 1st sigma y    
           #             for k in range(N_F):
           #                 first_sigma_y=first_sigma_y+g_FF*J_FF[n][k]*np.tanh(yt[k]);
                           
           #             yt[n]=yt[n-1]+(-yt[n-1]+first_sigma_y+second_sigma_y)/tau;

           #             second_sigma_x=second_sigma_x+np.dot(g_GF*J_GF[i-1][n],np.tanh(yt[n]));

           #     xt[i]=xt[i-1]+(-xt[i-1]+first_sigma_x+g_Gz*J_Gz[i-1]*z[t-1]+second_sigma_x)/tau;
               
               #r[i]=(np.tanh(xt[i]));
        r=xi(t=t); 
        #update weights every 2 steps
        if t%2==0:
            e_min[t]=np.subtract(np.dot(w[t-1].T,r),ft[t]);  
            P[t]=np.subtract(P[t-1],np.divide(np.dot(np.dot(np.dot(P[t-1],r), r.T), P[t-1]),np.add(1,np.dot(np.dot(r.T,P[t-1]),r)))) ;     

            w[t]=np.subtract(w[t-1],np.dot(e_min[t],np.dot(P[t],r)));

        else:
            e_min[t]=e_min[t-1];
            P[t]=P[t-1];
            w[t]=w[t-1];
        z[t]=np.dot(w[t].T,r); 

    return z;

z0=g_GG_simul();
plt.plot(z0);

import random
import numpy as np
import matplotlib.pyplot as plt
import scipy.sparse as sparse
from scipy import stats
#from functions import triangle, sinusoid



def triangle(time, amplitude, freq, noise = False, noise_intensity = 0.1):
    
    freq /= 2
        
    y = 4*freq*(time - np.floor(2*time*freq+0.5)/2/freq)*(-1)**(np.floor(2*time*freq+0.5))
    
    if noise: y+= np.random.uniform(-noise_intensity, noise_intensity, size = y.shape)

    return y


def square(time, amplitude, freq, noise = False, noise_intensity = 0.1):
    
    y = amplitude*np.sign(np.sin(freq*time))
    
    if noise: y+= np.random.uniform(-noise_intensity, noise_intensity, size = y.shape)
    
    return y
    


def sinusoid(time, amplitudes, freqs, noise = False, noise_intensity = 0.1):
    
    y = np.zeros(len(time))
    
    for n in range(len(amplitudes)):
        
        y += amplitudes[n]*np.sin(freqs[n]*np.pi*time)
        
    if noise: y+= np.random.uniform(-noise_intensity, noise_intensity, size = y.shape)
        
    return y


def lorenz_attractor(time, x_0, y_0, z_0, Prandtl_number = 10, Rayleigh_number = 28, beta = 8/3):
    
    dt = time[1] - time[0]
    
    x = [x_0]
    y = [y_0]
    z = [z_0]
    
    for t in range(len(time)):
        
        xp = (Prandtl_number * (y[t] - x[t])) * dt
        yp = (Rayleigh_number * x[t] - y[t] - x[t] * z[t]) * dt
        zp = (x[t] * y[t] - beta * z[t]) * dt
        
        x.append(x[-1] + xp)
        y.append(y[-1] + yp)
        z.append(z[-1] + zp)
        
    return x, y, z 

linewidth = 3;
fontsize = 14;
fontweight = 'bold';


N = 1000;
p = 0.1;
g = 1.5;				# g greater than 1 leads to chaotic networks.
alpha = 1.0;
nsecs = 1440;
dt = 0.1;
learn_every = 2;

scale = 1.0/np.sqrt(p*N);
rvs = stats.norm().rvs
Jg = sparse.random(N,N,p, data_rvs = rvs).todense()*g*scale
J_Gz=np.random.uniform(-1,1,(N,1));

nRec2Out = int(N)
neurons_recorded = 10



beta =0.5
wo = beta*np.random.randn(nRec2Out,1)/np.sqrt(N)		# synaptic strengths from internal pool to output unit
dwo = np.zeros((nRec2Out,1))
zidxs = np.arange(round(N))

print('   N: ' + str(N))
print('   g: '+str(g))
print('   p: '+ str(p))
print('   nRec2Out: '+ str(nRec2Out))
print('   alpha: '+ str(alpha))
print('   nsecs: '+ str(nsecs))
print('   learn_every: '+ str(learn_every))

pre_training=np.arange(0, nsecs/2, dt);

simtime = np.arange(0, nsecs, dt)
simtime_len = len(simtime)
simtime2 = np.arange(1*nsecs, 2*nsecs, dt)

amp = 1.3;
freq = 1/60;
ft = (amp/1.0)*np.sin(1.0*np.pi*freq*simtime) + (amp/2.0)*np.sin(2.0*np.pi*freq*simtime) + (amp/6.0)*np.sin(3.0*np.pi*freq*simtime) + (amp/3.0)*np.sin(4.0*np.pi*freq*simtime);
ft = ft/1.5;

ft2 = (amp/1.0)*np.sin(1.0*np.pi*freq*simtime2) + (amp/2.0)*np.sin(2.0*np.pi*freq*simtime2) + (amp/6.0)*np.sin(3.0*np.pi*freq*simtime2) + (amp/3.0)*np.sin(4.0*np.pi*freq*simtime2);
ft2 = ft2/1.5;

#ft=y_figE;

amp_figG = 1
freq_figG = 2/60
y_figG = square(simtime, amp_figG, freq_figG, noise = True, noise_intensity=0.01)


amp_figE = np.array([1, 1/4, 1/3, 1/15, 1/5, -1/14, 1/10, 1/12, 1/3, 1/2.5, 1/6, 1/2, -1/5, 1/30, 1/4, -1/10])
freq_figE = np.arange(1,17)/180
y_figE = sinusoid(simtime, amp_figE, freq_figE)
ft = y_figG

wo_pre_len = np.zeros((1,simtime_len))  

zt = np.zeros((1,simtime_len))
zpre=np.zeros((1,simtime_len));
zpt = np.zeros((1,simtime_len))


x0 = 0.5*np.random.randn(N,1);
z0 = 0.5*np.random.randn(1,1);

x = x0; 
r = np.tanh(x);
z = z0; 

plt.figure()
ti = 0;
Pz = (1.0/alpha)*np.identity(nRec2Out);



pick_random_neurons = np.random.randint(0, N/2, size = neurons_recorded)

randomlist = random.sample(range(0, N), 10)

#pre-training
neurons_pre= np.zeros((neurons_recorded,1))

n1_pre = np.array([])
n10_pre = np.array([])
n50_pre = np.array([])
n100_pre = np.array([])
n200_pre = np.array([])
n500_pre = np.array([])
n600_pre = np.array([])
n700_pre = np.array([])
n800_pre = np.array([])
n900_pre = np.array([])

for t in simtime:				# don't want to subtract time in indices

    if ti%(nsecs/2) == 0:
    	print('time(s): ' + str(t) +'.');
      #  plt.figure('fig.training');

    	plt.subplot(211);
    	plt.plot(simtime, ft.T, linewidth = linewidth, color = 'green');
    	plt.plot(simtime, zpre.T, linewidth = linewidth, color = 'red');
    	plt.title('Pre-train', fontsize = fontsize, fontweight = fontweight);
    	plt.xlabel('time (s)', fontsize = fontsize, fontweight = fontweight);
    	plt.ylabel('f, z', fontsize = fontsize, fontweight = fontweight);
    	plt.legend(['f', 'z']);
    	
    	plt.subplot(212);
    	plt.plot(simtime, wo_pre_len.T, linewidth = linewidth); 
    	plt.xlabel('time (s)', fontsize = fontsize, fontweight = fontweight);
    	plt.ylabel('|wo|', fontsize = fontsize, fontweight = fontweight);
    	plt.legend(['|wo|']);	
               
    	plt.pause(0.5);	
    
    # sim, so x(t) and r(t) are created.
    x = np.add(np.add(np.dot((1.0-dt), x), np.dot(np.dot(Jg,r),dt)), np.dot(np.dot(J_Gz,z),dt)); 
    
    n1_pre = np.append(n1_pre, x[1])
    n10_pre = np.append(n10_pre, x[50])
    n50_pre = np.append(n50_pre, x[1])
    n100_pre = np.append(n100_pre, x[100])
    n200_pre = np.append(n200_pre, x[20])
    n500_pre = np.append(n500_pre, x[500])
    n600_pre = np.append(n600_pre, x[600])
    n700_pre = np.append(n700_pre, x[700])
    n800_pre = np.append(n800_pre, x[800])
    n900_pre = np.append(n900_pre, x[900])

    r = np.tanh(x);
    z = np.dot(wo.T,r);
    
    zpre.T[ti] = z;
    wo_pre_len[0,ti] = np.sqrt(np.dot(wo.T, wo));	
    ti += 1;    
    neurons_pre = np.append(neurons_pre, np.array(r[randomlist]), axis = 1)

error_avg_pre = np.sum(np.abs(zpre-ft2))/simtime_len
print('Pre-training MAE: ' + str(error_avg_pre))



neurons = np.zeros((neurons_recorded,1))

ti=0;
wo_train_len = np.zeros((1,simtime_len));  

for t in simtime:

    if ti%(nsecs/2) == 0:
    	print('time(s): ' + str(t) +'.');
    	plt.subplot(211);
    	plt.plot(simtime, ft.T, linewidth = linewidth, color = 'green');
    	plt.plot(simtime, zt.T, linewidth = linewidth, color = 'red');
    	plt.title('Training', fontsize = fontsize, fontweight = fontweight);
    	plt.xlabel('time (s)', fontsize = fontsize, fontweight = fontweight);
    	plt.ylabel('f, z', fontsize = fontsize, fontweight = fontweight);
    	plt.legend(['f', 'z']);
    	
    	plt.subplot(212);
    	plt.plot(simtime, wo_train_len[-1].T, linewidth = linewidth); 
    	plt.xlabel('time(s)', fontsize = fontsize, fontweight = fontweight);
    	plt.ylabel('|wo|', fontsize = fontsize, fontweight = fontweight);
    	plt.legend(['|wo|'])
    	plt.pause(0.5)
        
    	if ti + nsecs/2 < len(simtime):
            plt.clf()
    
    # sim, so x(t) and r(t) are created.
    x = np.add(np.add(np.dot((1.0-dt), x), np.dot(np.dot(Jg,r),dt)), np.dot(np.dot(J_Gz,z),dt)); # note the y here.
    r = np.tanh(x);
    z = np.dot(wo.T,r);
    
    if ti % learn_every == 0:
    	# update inverse correlation matrix for the output unit
    	kz = np.dot(Pz,r);
    	rPrz = np.dot(r.T,kz);
    	cz = np.divide(1.0,np.add(1.0, rPrz));
    	Pz = np.subtract(Pz, np.dot(kz,np.dot(kz,cz).T));    
    	# update the error for the linear readout
    	e = np.subtract(z,ft[ti]);
    	# update the output weights
    	dwo = np.dot(np.dot(-float(e),kz),cz);
    	wo += dwo;


    # Store the output of the system.
    zt[0,ti] = z;
    wo_train_len[0,ti] = np.sqrt(np.dot(wo.T, wo));	
        
    ti += 1;	


    neurons = np.append(neurons,np.array(r[randomlist]), axis = 1)

    
        

error_avg = np.sum(np.abs(zt-ft))/simtime_len;
print('Training MAE: ' + str(error_avg));    
print('Now testing... please wait.');    

# Now test. 
ti = 0;
wo_post_len = np.zeros((1,simtime_len))  

neurons_test = np.zeros((neurons_recorded,1))

for t in simtime:				# don't want to subtract time in indices
    if ti%(nsecs/2) == 0:
    	 print('time(s): ' + str(t) +'.');
    	 plt.subplot(211);
    	 plt.plot(simtime, ft.T, linewidth = linewidth, color = 'green');
    	 plt.plot(simtime, zpt.T, linewidth = linewidth, color = 'red');
    	 plt.title('Test', fontsize = fontsize, fontweight = fontweight);
    	 plt.xlabel('time(s)', fontsize = fontsize, fontweight = fontweight);
    	 plt.ylabel('f, z', fontsize = fontsize, fontweight = fontweight);
    	 plt.legend(['f', 'z']);
    	
    	 plt.subplot(212);
    	 plt.plot(simtime, wo_post_len.T, linewidth = linewidth); 
    	 plt.xlabel('time(s)', fontsize = fontsize, fontweight = fontweight);
    	 plt.ylabel('|wo|', fontsize = fontsize, fontweight = fontweight);
    	 plt.legend(['|wo|']);	
    
       
    	 plt.pause(0.5);	     
    # sim, so x(t) and r(t) are created.
    x = np.add(np.add(np.dot((1.0-dt), x), np.dot(Jg, np.dot(r,dt))), np.dot(J_Gz,np.dot(z,dt))); 

    r = np.tanh(x);
    z = wo.T*r;
    wo_post_len[0,ti] = np.sqrt(np.dot(wo.T, wo));	

    zpt.T[ti] = z;
    
    neurons_test = np.append(neurons_test, np.array(r[randomlist]), axis = 1)
    
    ti += 1;    

error_avg = np.sum(np.abs(zpt-ft2))/simtime_len
print('Testing MAE: ' + str(error_avg))


plt.figure()
plt.subplot(211)
plt.plot(simtime, ft.T, linewidth = linewidth, color = 'green');
plt.plot(simtime, zt.T, linewidth = linewidth, color = 'red');
plt.title('training', fontsize = fontsize, fontweight =  fontweight);
plt.xlabel('time', fontsize =  fontsize, fontweight = fontweight);
plt.axis('tight')
plt.ylabel('f, z', fontsize = fontsize, fontweight = fontweight);
plt.legend(['f', 'z']);


plt.subplot(212)
plt.plot(simtime2, ft.T, linewidth = linewidth, color = 'green'); 
plt.plot(simtime2, zpt.T, linewidth = linewidth, color = 'red');
plt.axis('tight')
plt.title('simulation', fontsize = fontsize, fontweight = fontweight);
plt.xlabel('time', fontsize = fontsize, fontweight = fontweight);
plt.ylabel('f, z', fontsize = fontsize, fontweight = fontweight);
plt.legend(['f', 'z'])


def plot_activity(time, z, dw, neurons, title = None):
    
    nrows = 2 + neurons.shape[0]
    
    plt.figure()
    if title != None: plt.suptitle(title, fontweight='bold')
    plt.subplot(nrows, 1, 1)
    plt.plot(time, z, color = 'r')
    plt.axis('off')

    for row in range(neurons.shape[0]):
        plt.subplot(nrows, 1, 2 + row)
        plt.plot(time, neurons[row,:])
        plt.axis('off')    

    plt.subplot(nrows, 1, nrows)
    plt.plot(time[:-1],dw, color = 'orange')
    plt.axis('off')
    plt.text(-len(dw)*0.01, 0, '$|\dot w|$', fontweight = 'bold')
    
    
    
plot_activity(simtime, zpre.T,abs(np.diff(wo_pre_len).T), neurons_pre[:, 1:], title = 'Pre_learning')
plot_activity(simtime, zt.T, abs(np.diff(wo_train_len).T), neurons[:, 1:], title = 'Learning')
plot_activity(simtime, zpt.T, abs(np.diff(wo_post_len).T), neurons_test[:, 1:], title = 'Post_learning')






------------------END-----------------------








--------------------OLD CODE BELOW NOT WORKING----------------------------
---------------------------------------------------------------------
#unfinished: not sure if there is learning: e_min is not decreasing
import numpy as np;
import matplotlib.pyplot as plt;

g_GG=1.5;
g_Gz=1;
g_GF=0;
g_FF=1.2;
g_FG=1;
alpha=1;
N_G=10;
N_F=2;
p_GG=0.1;
p_Z=1;
p_FG=0.025;
p_FF=0.25;
p_GF=0.25;
T=1500;
np.random.seed(1);

P=np.zeros((T,N_G,N_G));
P[0]=np.divide(np.identity(N_G),alpha);
w=np.zeros((T,N_G));
w_nonzero=np.random.normal(0,1/(p_Z*N_G),(N_G));
for i in range(N_G):
    w[0][i]=np.random.choice([0,w_nonzero[i]]);


J_GG=np.random.normal(0,1/(p_Z*N_G),(N_G,N_G));
J_Gz=np.random.uniform(-1,1,(N_G));
J_FG=np.random.normal(0,1/(p_FG*N_G),(N_F,N_G));
J_FF=np.random.normal(0,1/(p_FF*N_F),(N_F,N_F));
J_GF=np.random.normal(0,1/(p_GF*N_F),(N_G,N_F));
tau=1;

#x0=0.1

#periodic function D
nsecs=T

simtime = np.arange(0, nsecs, tau)
simtime_len = len(simtime)
simtime2 = np.arange(1*nsecs, 2*nsecs, tau)

amp = 1.3;
freq = 1/60;
ft = (amp/1.0)*np.sin(1.0*np.pi*freq*simtime) + (amp/2.0)*np.sin(2.0*np.pi*freq*simtime) + (amp/6.0)*np.sin(3.0*np.pi*freq*simtime) + (amp/3.0)*np.sin(4.0*np.pi*freq*simtime);
ft = ft/1.5;




def first_sigma_x(d1=0):
    first_sigma_x=1;
    xt=np.ones(N_G);


    for i in range(N_G):

            first_sigma_x=first_sigma_x+g_GG*J_GG[d1-1][i]*np.tanh(xt[i]);
    return first_sigma_x;

def first_sigma_y(d1=0):
    first_sigma_y=1;
    yt=np.ones(N_F);
    for  i in range(1,N_F):
            first_sigma_y=first_sigma_y+g_FF*J_FF[d1-1][i]*np.tanh(yt[i-1]);
    return first_sigma_y;


def second_sigma_y(d1=0):
    second_sigma_y=1;
    xt=np.ones(N_G);
    for i in range(1,N_G):
        second_sigma_y=second_sigma_y+g_FG*J_FG[d1-1][i]*np.tan(xt[i]);
    return second_sigma_y ;

def ya(tau=tau):
    yt=np.ones(N_F);
    for i in range(1,N_F):
        yt[i]=yt[i-1]+(-yt[i-1]+first_sigma_y(i-1)+second_sigma_y(i-1))/tau;
    return yt;

def second_sigma_x(d1=0):
    yt=ya();
    second_sigma_x=1;
    for i in range(1,N_F):

        second_sigma_x=second_sigma_x+np.dot(np.dot(g_GF,J_GF[d1-1][i]),np.tanh(yt[i-1]));
        
    return second_sigma_x;

def xi(t=0,tau=tau):
    xt=np.ones(N_G);
    r=np.ones(N_G);
    z=np.zeros(T);

    for i in range(1,N_G):
        xt[i]=xt[i-1]+(-xt[i-1]+first_sigma_x(i)+g_Gz*J_Gz[i-1]*z[t-1]+second_sigma_x(i))/tau;
        r[i]=(np.tanh(xt[i]));
    return r;   

#N_I=0, so the last term is omitted
def g_GG_simul(N_G=N_G,N_F=N_F,tau=tau,T=T,ft=ft,P=P,w=w):
    # yt=np.ones(N_F);

    # xt=np.ones(N_G);
    # r=np.ones(N_G);
    z=np.zeros(T);
    #P=np.ones(T);
    e_min=np.zeros(T);
  #  e_min[0]=alpha*(f[1]-f[0])
    #loop for z
    for t in range(1,T):        

        
    #same thing as those fxns for sigma, y, and x
    #initialize sigmas in the two formula
        # first_sigma_x=1;
        # first_sigma_y=1;
        # second_sigma_y=1;
        # second_sigma_x=1;
        
        # if t>0:
           # #loop for xi and ri
           # for i in range(1,N_G):   
           #     #loop for 1st sigma x
           #     for j in range(N_G):
           #         first_sigma_x=first_sigma_x+g_GG*J_GG[i-1][j]*np.tanh(xt[j]);
           #         #loop for ya and 2nd sigma x
           #         for n in range(N_F):
           #             #loop for 2nd sigma y
           #             for m in range(N_G):
           #                 second_sigma_y=second_sigma_y+g_FG*J_FG[n][m]*np.tan(xt[m]);
           #             #loop for 1st sigma y    
           #             for k in range(N_F):
           #                 first_sigma_y=first_sigma_y+g_FF*J_FF[n][k]*np.tanh(yt[k]);
                           
           #             yt[n]=yt[n-1]+(-yt[n-1]+first_sigma_y+second_sigma_y)/tau;

           #             second_sigma_x=second_sigma_x+np.dot(g_GF*J_GF[i-1][n],np.tanh(yt[n]));

           #     xt[i]=xt[i-1]+(-xt[i-1]+first_sigma_x+g_Gz*J_Gz[i-1]*z[t-1]+second_sigma_x)/tau;
               
               #r[i]=(np.tanh(xt[i]));
        r=xi(t=t); 
        #update weights every 2 steps
        if t%2==0:
            e_min[t]=np.subtract(np.dot(w[t-1].T,r),ft[t]);  
            P[t]=np.subtract(P[t-1],np.divide(np.dot(np.dot(np.dot(P[t-1],r), r.T), P[t-1]),np.add(1,np.dot(np.dot(r.T,P[t-1]),r)))) ;     

            w[t]=np.subtract(w[t-1],np.dot(e_min[t],np.dot(P[t],r)));

        else:
            e_min[t]=e_min[t-1];
            P[t]=P[t-1];
            w[t]=w[t-1];
        z[t]=np.dot(w[t].T,r); 

    return z;

z0=g_GG_simul();
plt.plot(z0);
